#' Fitting PLS-DA Regression Model
#'
#'\code{plsda.fit} is used to fit a PLS-DA regression model.
#'
#' @usage
#' plsda.fit(formula,data,ncomp, var.select = F, nfold = 10)
#' @param
#' formula an object of class formula" (or one that can be coerced to that class): a symbolic description of the model
#' to be fitted. The details of model specification are given under ‘Details'.
#' @param
#' data the dataframe containing the variables in the model.
#' @param
#' ncomp optional, the number of components extracted in NIPALS algorithm. If ncomp is missing, then the number
#' of componenents is determined by Cross Validation with the function plsda.cv.
#' @param
#' var.select is \code{FALSE} by default, if \code{TRUE}, then the VIP will be calculated for each predictors with the
#' \code{plsda.vip} function. See 'Details' for specification about the VIP criterion.
#' @param
#' nfold is the number of folds used in cross validation. By default, \code{nfold = 10}.
#' @return
#' An object of class 'PLSDA' is a list containing at least the following components :
#' @return
#' \code{X} the original dataset containing the predictors.
#' \cr
#' \code{Y} the original vector of factors that is the variable to predict.
#' \cr
#' \code{level} the terms of the variable to predict.
#' \cr
#' \code{Xmeans} the means of each predictors.
#' \cr
#' \code{pls.coef}the loadings of the predictors, calculated by NIPALS algorithm in plsda.pls function.
#' \cr
#' \code{ncomp} the number of components used in plsda.pls.
#' \cr
#' \code{locoef} the coefficients of the logistic regression applied on the loadings of the predictors
#' to have the final coefficients of PLS-DA.
#' \cr
#' \code{plsda.coef} the final coefficients of the plsda, which are logit functions.
#' \cr
#'
#' @details
#' The PLS-DA regression model is used to fit values of a dataset with a large number of predictors (sometimes greater
#' than the number of observations). First of all, the NIPALS algorithm is performed on the data to get a prediction of
#' the target variable according to the components of the predictors. Then, a logistic regression is used on the loadings
#' of X in order to get the final coefficients of the PLS-DA regression model.The prediction will then use these
#' coefficients to perform a softmax function to transform them in probabilities of belonging to the different class for
#' for each observations.
#'
#' Some variables are more important than others for the prediction of Y. These variables can be identified by
#' calculating their VIP criterion. The \code{plsda.fit} can call the \code{plsda.vip} function to calculate
#' the VIP for each variable and select the variables important for predictions to build the model.
#' It is possible to plot the VIP by calling the plot.vip on \code{plsda.fit$VIP}.At least 2 variables must
#' be chosen to build the model. Hence, if only one variable is above the threshold, the algorithm choose
#' the 2 variables that have the greatest VIP.
#'
#' @examples
#' #ncomp is specified
#' fit.t1<-plsda.fit(Species~.,iris,3)
#' fit.t3<-plsda.fit(Species~.,iris,2, var.select = T)
#'
#'#ncomp is not specified, cross-validation will be performed automatically
#'fit.t2<-plsda.fit(Species~.,iris, nfold = 30)
#'fit.t4<-plsda.fit(Species~.,iris, var.select = T)
#'fit.t5<-plsda.fit(Species~.,iris)


# APPRENTISSAGE ET CREATION DU MODELE PLSDA
plsda.fit<-function(formula,data,ncomp, var.select = F, nfold = 10){

  #VÃ©rification que l'entrÃ©e est bien une formule Y~X
  if(plyr::is.formula(formula)==F){
    stop("formula must be R formula !")
  }

  #Récupération des X et Y
  X <- model.matrix(formula, data = data)
  X <- X[,-1] #suppression de l'intercept
  Y <- model.response(model.frame(formula, data = data))

  # CohÃ©rence de Y
  if(is.factor(Y)==F && is.vector(Y)==F){ stop("Dimension de Y incorrecte !") }

  if(!(is.factor(Y))){ Y <- as.factor(Y) }

  # RÃ©cupÃ©ration des moyennes
  Xmeans <- colMeans(X)

  # MetadonnnÃ©es X et Y
  l<-levels(Y)
  nl<-nlevels(Y)

  # si le nombre de composante est indique
  cv <- F
  #sinon :
  if(missing(ncomp)){
    # DÃ©termination automatique du nombre de composantes par Validation Croisee
    CV <- plsda.cv(formula, data, nfold = nfold)
    ncomp <- CV$ncomp
    cv <- T
  }

  # Algorithme NIPALS avec le nombre de composante selectionnee
  pls<-plsda.pls(formula = formula, data = data,ncomp, center = T)
  if(var.select == TRUE){
    vip <- plsda.vip(pls)
    new.data <- data.frame(vip$newX,Y)
    pls <- plsda.pls(Y~., data = new.data, ncomp, center = T)
  }

  # Regression logistique
  lo<-nnet::multinom(Y~.,pls$TrainPlsData,trace=F)

  # Coefficients de la rÃ©gression logistique (fonctions logit)
  cl<-coef(lo)
  lo_coef <- c(1,rep_len(0,ncol(cl)-1))
  lo_coef<-rbind(lo_coef,cl)
  rownames(lo_coef)<-l

  # Coefficients finaux (fonction logit)
  plsda_coef <- lo_coef[,-1] %*% t(pls$Xloadings)
  plsda_coef<- t(cbind(plsda_coef,"Intercept"=as.vector(lo_coef[,1])))

  # Liste de retour
  object=list("X"=X,
              "Y"=Y,
              "level"=l,
              "Xmeans"=Xmeans,
              "pls.coef"=pls$Xloadings,
              "pls.Xscores"=pls$Xscores,
              "ncomp"=ncomp,
              "lo.coef"=lo_coef,
              "plsda.coef"=plsda_coef)
  #selection de variables
  if(var.select == T){
    object[["X"]] <- vip$newX
    object[["VIP"]] <- vip
    object[["Xmeans"]] <- colMeans(vip$newX)
  }
  if(cv == T){
    object[["CV"]] <- CV
  }
  # DÃ©finition de l'objet
  class(object)<-"PLSDA"
  return(object)
}

